{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "76b1744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63138138",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    config = config.get('config', {})\n",
    "assert config, \"Config file is empty or not loaded properly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1f43d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 6,\n",
       " 'num_heads': 8,\n",
       " 'd_model': 512,\n",
       " 'd_ff': 2048,\n",
       " 'dropout_rate': 0.1,\n",
       " 'context_length': 512,\n",
       " 'vocab_size': 50257,\n",
       " 'embedding_dim': 512}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bfb5f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.d_per_head = config['d_model'] // self.num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.Wk = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.Wv = nn.Linear(config['d_model'], config['d_model'])\n",
    "        self.register_buffer(\n",
    "            name=\"attention_mask\", \n",
    "            tensor=torch.triu(torch.ones(config['context_length'], config['context_length']), diagonal=1),\n",
    "            persistent=False    # Dont need a state\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "        self.projection = nn.Linear(config['d_model'], config['d_model'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch x num_tokens x d_model)\n",
    "        batch, num_tokens, d_model = x.shape\n",
    "        \"\"\"\n",
    "        queries = self.Wq(x)    # b x num_tokens x d_model\n",
    "        keys = self.Wk(x)       # b x num_tokens x d_model\n",
    "        values = self.Wv(x)     # b x num_tokens x d_model\n",
    "\n",
    "        queries = queries.reshape(batch, num_tokens, self.num_heads, self.d_per_head)     # b x num_tokens x num_heads x self.d_per_head\n",
    "        keys = keys.reshape(batch, num_tokens, self.num_heads, self.d_per_head)        # b x num_tokens x num_heads x self.d_per_head\n",
    "        values = values.reshape(batch, num_tokens, self.num_heads, self.d_per_head)      # b x num_tokens x num_heads x self.d_per_head\n",
    "\n",
    "        queries = queries.transpose(1, 2)   # b x num_heads x num_tokens x self.d_per_head\n",
    "        keys = keys.transpose(1, 2)         # b x num_heads x num_tokens x self.d_per_head\n",
    "        values = values.transpose(1, 2)     # b x num_heads x num_tokens x self.d_per_head\n",
    "        \"\"\"\n",
    "        queries = self.Wq(x).reshape(batch, num_tokens, self.num_heads, self.d_per_head).transpose(1, 2)\n",
    "        keys = self.Wk(x).reshape(batch, num_tokens, self.num_heads, self.d_per_head).transpose(1, 2)\n",
    "        values = self.Wv(x).reshape(batch, num_tokens, self.num_heads, self.d_per_head).transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(-1, -2)     # b x num_heads x num_tokens x num_tokens\n",
    "        attention_scores = attention_scores / (self.d_per_head ** 0.5)\n",
    "        # Now mask the attention scores and pass through dropout\n",
    "        attention_scores.masked_fill_(self.attention_mask[:num_tokens, :num_tokens].bool(), -torch.inf) # this is causal attention\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)       \n",
    "\n",
    "        # Get context vectors\n",
    "        context_vector = attention_scores @ values      # b x num_heads x num_tokens x self.d_per_head\n",
    "        context_vector = context_vector.transpose(1, 2) # b x num_tokens x num_heads x self.d_per_head\n",
    "\n",
    "        context_vector = context_vector.reshape(batch, num_tokens, -1)\n",
    "        return self.projection(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1b5b5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['d_model'])\n",
    "        self.attention = MaskedMultiHeadAttention(config)\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "        self.ln_2 = nn.LayerNorm(config['d_model'])\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_ff']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config['d_ff'], config['d_model'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.ln_1(x)))\n",
    "        x = x + self.dropout(self.ff(self.ln_2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0ee31c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        self.pos_embedding = nn.Embedding(config['context_length'], config['embedding_dim'])\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "        # Adding this -> Brand new to the architecture\n",
    "        self.dim_project = None\n",
    "        if config['embedding_dim'] != config['d_model']:\n",
    "            self.dim_project = nn.Sequential(\n",
    "                nn.Linear(config['embedding_dim'], config['d_model']), \n",
    "                nn.GELU()\n",
    "            )\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config['num_layers'])\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
    "        self.fc_out = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "        # Tying weights\n",
    "        self.fc_out.weight = self.token_embedding.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target:torch.Tensor = None):\n",
    "        batch_size, n_tokens = x.shape\n",
    "        x = self.token_embedding(x) \n",
    "        x += self.pos_embedding(torch.arange(n_tokens, device=x.device)).unsqueeze(0)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.dim_project:\n",
    "            x = self.dim_project(x)\n",
    "\n",
    "        for tx_block in self.transformer_blocks:\n",
    "            x = tx_block(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        if target is None:\n",
    "            logits = self.fc_out(x[:, -1, :])\n",
    "            return logits, None\n",
    "        else:\n",
    "            logits = self.fc_out(x)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), target.reshape(-1), ignore_index=-100)\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a74b4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "efeb816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, config['vocab_size'], size=(4, 16))\n",
    "y = torch.randint(0, config['vocab_size'], size=(4, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b6413a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50257]) tensor(312.0105, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "print(logits.shape, loss if loss else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4bf5fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight 25731584\n",
      "pos_embedding.weight 262144\n",
      "transformer_blocks.0.ln_1.weight 512\n",
      "transformer_blocks.0.ln_1.bias 512\n",
      "transformer_blocks.0.attention.Wq.weight 262144\n",
      "transformer_blocks.0.attention.Wq.bias 512\n",
      "transformer_blocks.0.attention.Wk.weight 262144\n",
      "transformer_blocks.0.attention.Wk.bias 512\n",
      "transformer_blocks.0.attention.Wv.weight 262144\n",
      "transformer_blocks.0.attention.Wv.bias 512\n",
      "transformer_blocks.0.attention.projection.weight 262144\n",
      "transformer_blocks.0.attention.projection.bias 512\n",
      "transformer_blocks.0.ln_2.weight 512\n",
      "transformer_blocks.0.ln_2.bias 512\n",
      "transformer_blocks.0.ff.0.weight 1048576\n",
      "transformer_blocks.0.ff.0.bias 2048\n",
      "transformer_blocks.0.ff.2.weight 1048576\n",
      "transformer_blocks.0.ff.2.bias 512\n",
      "transformer_blocks.1.ln_1.weight 512\n",
      "transformer_blocks.1.ln_1.bias 512\n",
      "transformer_blocks.1.attention.Wq.weight 262144\n",
      "transformer_blocks.1.attention.Wq.bias 512\n",
      "transformer_blocks.1.attention.Wk.weight 262144\n",
      "transformer_blocks.1.attention.Wk.bias 512\n",
      "transformer_blocks.1.attention.Wv.weight 262144\n",
      "transformer_blocks.1.attention.Wv.bias 512\n",
      "transformer_blocks.1.attention.projection.weight 262144\n",
      "transformer_blocks.1.attention.projection.bias 512\n",
      "transformer_blocks.1.ln_2.weight 512\n",
      "transformer_blocks.1.ln_2.bias 512\n",
      "transformer_blocks.1.ff.0.weight 1048576\n",
      "transformer_blocks.1.ff.0.bias 2048\n",
      "transformer_blocks.1.ff.2.weight 1048576\n",
      "transformer_blocks.1.ff.2.bias 512\n",
      "transformer_blocks.2.ln_1.weight 512\n",
      "transformer_blocks.2.ln_1.bias 512\n",
      "transformer_blocks.2.attention.Wq.weight 262144\n",
      "transformer_blocks.2.attention.Wq.bias 512\n",
      "transformer_blocks.2.attention.Wk.weight 262144\n",
      "transformer_blocks.2.attention.Wk.bias 512\n",
      "transformer_blocks.2.attention.Wv.weight 262144\n",
      "transformer_blocks.2.attention.Wv.bias 512\n",
      "transformer_blocks.2.attention.projection.weight 262144\n",
      "transformer_blocks.2.attention.projection.bias 512\n",
      "transformer_blocks.2.ln_2.weight 512\n",
      "transformer_blocks.2.ln_2.bias 512\n",
      "transformer_blocks.2.ff.0.weight 1048576\n",
      "transformer_blocks.2.ff.0.bias 2048\n",
      "transformer_blocks.2.ff.2.weight 1048576\n",
      "transformer_blocks.2.ff.2.bias 512\n",
      "transformer_blocks.3.ln_1.weight 512\n",
      "transformer_blocks.3.ln_1.bias 512\n",
      "transformer_blocks.3.attention.Wq.weight 262144\n",
      "transformer_blocks.3.attention.Wq.bias 512\n",
      "transformer_blocks.3.attention.Wk.weight 262144\n",
      "transformer_blocks.3.attention.Wk.bias 512\n",
      "transformer_blocks.3.attention.Wv.weight 262144\n",
      "transformer_blocks.3.attention.Wv.bias 512\n",
      "transformer_blocks.3.attention.projection.weight 262144\n",
      "transformer_blocks.3.attention.projection.bias 512\n",
      "transformer_blocks.3.ln_2.weight 512\n",
      "transformer_blocks.3.ln_2.bias 512\n",
      "transformer_blocks.3.ff.0.weight 1048576\n",
      "transformer_blocks.3.ff.0.bias 2048\n",
      "transformer_blocks.3.ff.2.weight 1048576\n",
      "transformer_blocks.3.ff.2.bias 512\n",
      "transformer_blocks.4.ln_1.weight 512\n",
      "transformer_blocks.4.ln_1.bias 512\n",
      "transformer_blocks.4.attention.Wq.weight 262144\n",
      "transformer_blocks.4.attention.Wq.bias 512\n",
      "transformer_blocks.4.attention.Wk.weight 262144\n",
      "transformer_blocks.4.attention.Wk.bias 512\n",
      "transformer_blocks.4.attention.Wv.weight 262144\n",
      "transformer_blocks.4.attention.Wv.bias 512\n",
      "transformer_blocks.4.attention.projection.weight 262144\n",
      "transformer_blocks.4.attention.projection.bias 512\n",
      "transformer_blocks.4.ln_2.weight 512\n",
      "transformer_blocks.4.ln_2.bias 512\n",
      "transformer_blocks.4.ff.0.weight 1048576\n",
      "transformer_blocks.4.ff.0.bias 2048\n",
      "transformer_blocks.4.ff.2.weight 1048576\n",
      "transformer_blocks.4.ff.2.bias 512\n",
      "transformer_blocks.5.ln_1.weight 512\n",
      "transformer_blocks.5.ln_1.bias 512\n",
      "transformer_blocks.5.attention.Wq.weight 262144\n",
      "transformer_blocks.5.attention.Wq.bias 512\n",
      "transformer_blocks.5.attention.Wk.weight 262144\n",
      "transformer_blocks.5.attention.Wk.bias 512\n",
      "transformer_blocks.5.attention.Wv.weight 262144\n",
      "transformer_blocks.5.attention.Wv.bias 512\n",
      "transformer_blocks.5.attention.projection.weight 262144\n",
      "transformer_blocks.5.attention.projection.bias 512\n",
      "transformer_blocks.5.ln_2.weight 512\n",
      "transformer_blocks.5.ln_2.bias 512\n",
      "transformer_blocks.5.ff.0.weight 1048576\n",
      "transformer_blocks.5.ff.0.bias 2048\n",
      "transformer_blocks.5.ff.2.weight 1048576\n",
      "transformer_blocks.5.ff.2.bias 512\n",
      "ln_f.weight 512\n",
      "ln_f.bias 512\n",
      "fc_out.bias 50257\n",
      "44.959313\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    print(name, param.numel())\n",
    "print(total_params / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca457db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
